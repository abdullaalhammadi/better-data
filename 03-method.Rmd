# The Key Aspects of Data

In this chapter, I will explain why these aspects of data are crucial to providing good results.

## Relevance

Relevance in this context is essentially how fitting the data is with your current goals. For example, in our case, the data is relevant because we are looking to explore the data to find any interesting relationships or occurrences that may help us come up with solutions or insights to improve road safety. This is the data we currently have.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
Sys.setlocale("LC_CTYPE","arabic")
library(readxl)
library(knitr)
library(tidyverse)
workfile <- read_excel("E:\\Fedora Media Writer\\Miscellanous\\FCSA project-20200806T165957Z-001\\FCSA project\\Work files\\traffic_incidents.xlsx")
```

```{r, echo=FALSE}
kable(head(workfile))
```


In our case, the data is relevant because we are working with what is available and not asking certain questions or setting specific goals by ourselves. To ensure that data is relevant, the entities that collect and provide the data should know what the users of the data are looking for. Collecting random data that may or may not be useful is not an effective way of providing data, since the efforts and resources used to gather and provide such data may be fruitless.

## Reliability

Reliability is how much can the data be trusted to provide accurate and meaningful results. In our case for example, the data passed the initial reliability test, which is tests the integrity of the data. I wrote my own script to determine whether there were any inconsistencies between the cells.
```{r}
check_doubles <- function(x){

  x <- x %>% 
    group_by(acci_id) %>% 
    filter(n_distinct(acci_name) > 1)

  if(nrow(x) > 1) {
    print(x)
  }
    else print("There are no inconsistencies between the cells")
}
```
This is a script that checks that each entry has a unique ID, which ensures that there are no duplicate entries or mismatched entries. However, the data proved to be unreliable because when we checked the completeness of the data, this is what we can see:
```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(lubridate)
library(tidyverse)
```

```{r}
workfile %>% mutate(date = date(acci_time)) %>% count(is.na(date) == TRUE)
```

We can see that more than half of the entries have no dates. This is a huge problem because dates can provide a lot of insights that is now very difficult to obtain. For example, we could have used a weather API to get the weather conditions on the dates, and see the effect that the weather conditions might have on road safety. Also there is another interesting observation.

```{r}
workfile %>% mutate(incident_date = date(acci_time)) %>%
    mutate(incident_day = day(acci_time), Incident_month = month(acci_time)) %>%
    group_by(Incident_month) %>% count(incident_day) %>%
    count(Incident_month)
```

We can see that the that all the data collected is only from 3 to 4 days from each month. Whether this was an error or not, this can pose a lot of problems when exploring the data, and it is enough of a problem to make a lot of analysis' useless. All of these errors all in all make the dataset unreliable. Yes, it is still possible to explore the data and possibly come up with conclusions, but these conclusions will have a very poor basis, making the results from the conclusion unreliable.

## Coverage

Coverage can be thought of as how much information the data **_can_** give you. The amount of data that a dataset includes is affected by several factors, including the feasibility of data collection and relevance.

### Using Technology to Cover More Data

In our case, modern technology could have been used to collect much more data, and the collected data could cover a lot of other variables such as the road features(bridge, junction, intersection, etc.), weather conditions (foggy, rainy, etc.), and much more. For example, Dubai Police receives traffic incidents via their mobile app. Meaning that the whole data collection process can be automated, including automatically obtaining the weather conditions on that day through the AccuWeather API, and the road features on the provided coordinates, which could be done through OpenStreetMap for example.

In this dataset, we could've added more coverage by covering the severity of the incidents. We were able to do that by splitting the `acci_type` into two columns: `Incident_type` and `severity`.

```{r, message=FALSE, warning=FALSE}
workfile <- workfile %>% separate(acci_name, c("Incident_type", "severity"), sep = "- |-")
kable(head(workfile))
```

Now, it is possible to explore the relationships between the severity of the incidents and other variables. By increasing the coverage of the data, there are much more insights that can be gathered than before.

## Consistency

This is a very simple aspect, but is quite often overlooked. Having specific guidelines and standards to follow when collecting and using data is important to have consistent results. For example, if we collect data but with different variables next year, it can be difficult to track and compare changes between now and then. Therefore, a solid framework that allows for consistent results is important to maximize the usefulness of the data.