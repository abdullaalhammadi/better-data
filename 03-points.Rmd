# The Key Aspects of Data

## Relevance

Relevance in this context is essentially how fitting the data is with a task's set targets. For example in this case, the data is relevant because the task at hand is to explore the data to find any interesting relationships or occurrences that may help with providing solutions or insights to improve road safety. This is a glimpse of the dataset.

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
Sys.setlocale("LC_CTYPE","arabic")
library(readxl)
library(knitr)
library(tidyverse)
workfile <- read_excel("E:\\Fedora Media Writer\\Miscellanous\\FCSA project-20200806T165957Z-001\\FCSA project\\Work files\\traffic_incidents.xlsx")
```

```{r, echo=FALSE}
kable(head(workfile))
```


In this case, the data is relevant because task provides enough flexibility to work with what is available and not asking certain questions or setting specific goals. To ensure that data is relevant, the entities that collect and provide the data should know what the users of the data are looking for. Collecting random data that may or may not be useful is not an effective way of providing data, since the efforts and resources used to gather and provide such data may be fruitless. A good example of ensuring that data is relevant is this hypothetical scenario:

> a person, X, is  tasked with gathering data for the ABC Corp. annual performance report. Instead of collecting all the data that is possible, X decides to make the most out of his time by ensuring that the data that is intends to collect is relevant. So X meets with the committee that will be writing the report, and asks them what data is needed to come up with the metrics required for a proper report that is valuable for stakeholders. By doing this, X has saved valuable time and effort as well as being much more productive than just blindly collecting data, in the hopes that the data may be relevant for the use case.

## Reliability

Reliability is how much can the data be trusted to provide accurate and meaningful results. In this case for example, the data passed the initial reliability test, which tests the integrity of the data. This script is used to determine whether there were any inconsistencies between the cells.
```{r, echo=TRUE}
check_doubles <- function(x){

  x <- x %>% 
    group_by(acci_id) %>% 
    filter(n_distinct(acci_name) > 1)

  if(nrow(x) > 1) {
    print(x)
  }
    else print("There are no inconsistencies between the cells")
}
```
This is a script that checks that each entry has a unique ID, which ensures that there are no duplicate entries or mismatched entries. However, the data proved to be unreliable because when the verifying the completeness of the data, this is what can be seen:
```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(lubridate)
library(tidyverse)
```

```{r, echo=TRUE}
workfile %>% mutate(date = date(acci_time)) %>% count(is.na(date) == TRUE)
```

It can be seen that more than half of the entries have no dates. This is a huge problem because dates can provide a lot of insights that is now very difficult to obtain. For example, a weather API could have been used to get the weather conditions on the dates, and study the effect that the weather conditions might have on road safety. There is another interesting observation.
```{r, echo=TRUE}
workfile %>% mutate(incident_date = date(acci_time)) %>%
    mutate(incident_day = day(acci_time), Incident_month = month(acci_time)) %>%
    group_by(Incident_month) %>% count(incident_day) %>%
    count(Incident_month)
```

All the data collected is only from 3 to 4 days from each month. Whether this was an error or not, this can pose a lot of problems when exploring the data, and it is enough of a problem to make a lot of analysis useless. These issues when combined make the dataset unreliable. Yes, it is still possible to explore the data and possibly come up with conclusions, but these conclusions will have a very poor basis, making the results unreliable. A possible solution is automating the whole data collection process to reduce the possibility of collecting faulty data. Using a mobile app to submit incident reports is a solution that is currently being implemented by Dubai Police, and this solution is effective in ensuring the reliability of data because the data automatically registers the exact date and time of an incident.

## Coverage   

Coverage can be thought of as how much information the data **_can_** give. The amount of data that a dataset includes is affected by several factors, including the feasibility of data collection and relevance.

### Using Technology to Cover More Data

In this case, modern technology could have been used to collect much more data, and the collected data could cover a lot of other variables such as the road features(bridge, junction, intersection, etc.), weather conditions (foggy, rainy, etc.), and much more. For example, Dubai Police receives traffic incidents that are submuitted via a mobile app. Meaning that the whole data collection process can be automated, including automatically obtaining the weather conditions on that day through the API's such as Accuweather, and the road features on the provided coordinates, which could be done through OpenStreetMap for example.

In this dataset, It is possible to increase the coverage by adding a new variable, the severity of the incidents. Because the severity of the incidents is entered with the accident type. So, by splitting the `acci_type` into two columns: `Incident_type` and `severity`, it is possible to determine the severity of the incidents and further explore the variable.

```{r, message=FALSE, warning=FALSE}
workfile <- workfile %>% separate(acci_name, c("Incident_type", "severity"), sep = "- |-")
kable(head(workfile))
```

Now, it is possible to explore the relationships between the severity of the incidents and other variables. By increasing the coverage of the data, there are much more insights that can be gathered than before.

## Consistency

This is a very simple aspect, but is quite often overlooked. Having specific guidelines and standards to follow when collecting and using data is important to have consistent results. For example, if the data is collected but with different variables next year, it can be difficult to track and compare changes between now and then. Therefore, a solid framework that allows for both consistent results and the improvement of data quality is important to maximize the usefulness of the data.  

