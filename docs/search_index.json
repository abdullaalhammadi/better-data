[
["index.html", "Better Data and Better Results Chapter 1 Introduction 1.1 The Purpose 1.2 The Case", " Better Data and Better Results Abdulla Alhammadi | عبدالله الحمادي 2020-08-08 Chapter 1 Introduction 1.1 The Purpose I created this project as a personal initiative of how setting better standards can help utilize modern technology to better collect and analyze data for specific applications, as well as how to make the most out of public data. In this case, we will look into developing the standards for collecting public data, and how to make the most with what we currently have to create better insights. In this case, for public policy. 1.2 The Case The case at hand is an example of public data that is available on Bayanat.ae. It is a dataset that includes data about traffic incidents that were reported to the Dubai Police operation contact center. We will be using this dataset to look for insights on how to improve road safety. In the following pages, I will explain why I chose this dataset and how it is a good example for the ideas that will be mentioned in the following chapters. "],
["the-dataset.html", "Chapter 2 The Dataset 2.1 Why this dataset? 2.2 What will be covered?", " Chapter 2 The Dataset 2.1 Why this dataset? I chose this data set specifically because it covers most of the ideas that I will bring up. Do note that this dataset is not a cherry-picked one, but it offers a lot to work with. This dataset provides a wide variety of types of observations (date and time, geographical data, nominal data, ordinal data, etc.) as well as a lot of room for improvement and utilization. 2.2 What will be covered? The ideas that will be covered are: The 4 “key” aspects of data, which are: Relevance Reliability Coverage Consistency Setting the right standards Making the most of what is available Maximizing the effectiveness of data These are not the definitive and final points that make or break the data, but more of a general guideline of best practices and a grasp of positive though process when working with such tasks. In the coming chapters, we will go further into detail regarding the ideas listed above. "],
["the-key-aspects-of-data.html", "Chapter 3 The Key Aspects of Data 3.1 Relevance 3.2 Reliability 3.3 Coverage 3.4 Consistency", " Chapter 3 The Key Aspects of Data In this chapter, I will explain why these aspects of data are crucial to providing good results. 3.1 Relevance Relevance in this context is essentially how fitting the data is with your current goals. For example, in our case, the data is relevant because we are looking to explore the data to find any interesting relationships or occurrences that may help us come up with solutions or insights to improve road safety. This is the data we currently have. ## [1] &quot;Arabic_Saudi Arabia.1256&quot; acci_id acci_time acci_name acci_x acci_y 3545008155 2019-06-10 11:59:50 حادث دهس امراة- بليغ 25.28002 55.35302 3545009716 2019-06-10 12:04:19 حادث صدم رصيف- بسيط 25.25702 55.29077 3545011689 2019-06-10 12:09:48 حادث صدم جدار- بسيط 25.17389 55.40356 3545013868 2019-06-10 12:18:18 حادث ضد مجهول- بسيط 25.26867 55.32277 3544995157 2019-06-10 11:22:00 حادث اصطدام بين سيارتين- بسيط 25.26062 55.31896 3545003866 2019-06-10 11:47:55 حادث صدم جدار- بسيط 25.08618 55.40152 In our case, the data is relevant because we are working with what is available and not asking certain questions or setting specific goals by ourselves. To ensure that data is relevant, the entities that collect and provide the data should know what the users of the data are looking for. Collecting random data that may or may not be useful is not an effective way of providing data, since the efforts and resources used to gather and provide such data may be fruitless. 3.2 Reliability Reliability is how much can the data be trusted to provide accurate and meaningful results. In our case for example, the data passed the initial reliability test, which is tests the integrity of the data. I wrote my own script to determine whether there were any inconsistencies between the cells. check_doubles &lt;- function(x){ x &lt;- x %&gt;% group_by(acci_id) %&gt;% filter(n_distinct(acci_name) &gt; 1) if(nrow(x) &gt; 1) { print(x) } else print(&quot;There are no inconsistencies between the cells&quot;) } This is a script that checks that each entry has a unique ID, which ensures that there are no duplicate entries or mismatched entries. However, the data proved to be unreliable because when we checked the completeness of the data, this is what we can see: workfile %&gt;% mutate(date = date(acci_time)) %&gt;% count(is.na(date) == TRUE) ## # A tibble: 2 x 2 ## `is.na(date) == TRUE` n ## &lt;lgl&gt; &lt;int&gt; ## 1 FALSE 10078 ## 2 TRUE 13743 We can see that more than half of the entries have no dates. This is a huge problem because dates can provide a lot of insights that is now very difficult to obtain. For example, we could have used a weather API to get the weather conditions on the dates, and see the effect that the weather conditions might have on road safety. Also there is another interesting observation. workfile %&gt;% mutate(incident_date = date(acci_time)) %&gt;% mutate(incident_day = day(acci_time), Incident_month = month(acci_time)) %&gt;% group_by(Incident_month) %&gt;% count(incident_day) %&gt;% count(Incident_month) ## # A tibble: 13 x 2 ## # Groups: Incident_month [13] ## Incident_month n ## &lt;dbl&gt; &lt;int&gt; ## 1 1 3 ## 2 2 3 ## 3 3 3 ## 4 4 4 ## 5 5 3 ## 6 6 4 ## 7 7 3 ## 8 8 3 ## 9 9 3 ## 10 10 3 ## 11 11 3 ## 12 12 3 ## 13 NA 1 We can see that the that all the data collected is only from 3 to 4 days from each month. Whether this was an error or not, this can pose a lot of problems when exploring the data, and it is enough of a problem to make a lot of analysis’ useless. All of these errors all in all make the dataset unreliable. Yes, it is still possible to explore the data and possibly come up with conclusions, but these conclusions will have a very poor basis, making the results from the conclusion unreliable. 3.3 Coverage Coverage can be thought of as how much information the data can give you. The amount of data that a dataset includes is affected by several factors, including the feasibility of data collection and relevance. 3.3.1 Using Technology to Cover More Data In our case, modern technology could have been used to collect much more data, and the collected data could cover a lot of other variables such as the road features(bridge, junction, intersection, etc.), weather conditions (foggy, rainy, etc.), and much more. For example, Dubai Police receives traffic incidents via their mobile app. Meaning that the whole data collection process can be automated, including automatically obtaining the weather conditions on that day through the AccuWeather API, and the road features on the provided coordinates, which could be done through OpenStreetMap for example. In this dataset, we could’ve added more coverage by covering the severity of the incidents. We were able to do that by splitting the acci_type into two columns: Incident_type and severity. workfile &lt;- workfile %&gt;% separate(acci_name, c(&quot;Incident_type&quot;, &quot;severity&quot;), sep = &quot;- |-&quot;) workfile ## # A tibble: 23,821 x 6 ## acci_id acci_time Incident_type severity acci_x acci_y ## &lt;dbl&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3545008155 2019-06-10 11:59:50 حادث دهس امراة بليغ 25.3 55.4 ## 2 3545009716 2019-06-10 12:04:19 حادث صدم رصيف بسيط 25.3 55.3 ## 3 3545011689 2019-06-10 12:09:48 حادث صدم جدار بسيط 25.2 55.4 ## 4 3545013868 2019-06-10 12:18:18 حادث ضد مجهول بسيط 25.3 55.3 ## 5 3544995157 2019-06-10 11:22:00 حادث اصطدام بين سيارتين بسيط 25.3 55.3 ## 6 3545003866 2019-06-10 11:47:55 حادث صدم جدار بسيط 25.1 55.4 ## 7 3544986692 2019-06-10 10:53:35 حادث اصطدام بين سيارتين بسيط 25.2 55.5 ## 8 3544991407 2019-06-10 11:11:04 حادث صدم رصيف بسيط 25.1 55.4 ## 9 3544994531 2019-06-10 11:19:49 حادث ضد مجهول بسيط 25.2 55.3 ## 10 3544998937 2019-06-10 11:33:00 حادث صدم دراجة بسيط 25.1 55.2 ## # ... with 23,811 more rows Now, it is possible to explore the relationships between the severity of the incidents and other variables. By increasing the coverage of the data, there are much more insights that can be gathered than before. 3.4 Consistency This is a very simple aspect, but is quite often overlooked. Having specific guidelines and standards to follow when collecting and using data is important to have consistent results. For example, if we collect data but with different variables next year, it can be difficult to track and compare changes between now and then. Therefore, a solid framework that allows for consistent results is important to maximize the usefulness of the data. ## [1] &quot;Arabic_Saudi Arabia.1256&quot; ## # A tibble: 2 x 2 ## `is.na(date) == TRUE` n ## &lt;lgl&gt; &lt;int&gt; ## 1 FALSE 10078 ## 2 TRUE 13743 ## # A tibble: 13 x 2 ## # Groups: Incident_month [13] ## Incident_month n ## &lt;dbl&gt; &lt;int&gt; ## 1 1 3 ## 2 2 3 ## 3 3 3 ## 4 4 4 ## 5 5 3 ## 6 6 4 ## 7 7 3 ## 8 8 3 ## 9 9 3 ## 10 10 3 ## 11 11 3 ## 12 12 3 ## 13 NA 1 "],
["foundations.html", "Chapter 4 Foundations 4.1 Standards 4.2 Making the Most out of Things 4.3 Maximizing the effectiveness of data", " Chapter 4 Foundations 4.1 Standards Setting standards for data collection is vital for providing equal access to a wide variety of users as well as a wide variety of applications. Specific standards that are to be followed in the data collection process are essential to creating a robust foundation that makes data much more impactful. For example, setting standards for the key aspects of data can make sure that the results are consistent as well as reproducible for many other applications. In this dataset, setting standards for data entry could have made the dataset less difficult to work with, as it can be seen from the instance where we had to create the two columns Incident_type and severity from the previous chapter because the two scopes of data were coerced into one variable, which had to be adjusted to become useful. Another standard, such as adding indexes, is very helpful because it helps us better identify nominal data, as well as adding a common key for future purposes, such as translation. In this case, we added an ID for severity, in case we would like to manipulate the data later on. library(tidyverse) library(knitr) dfcount &lt;- workfile %&gt;% count(severity) indexlegend &lt;- dfcount %&gt;% dplyr::select(severity) %&gt;% na.omit() %&gt;% mutate(severity_ID = c(2, 3, 1)) workfile &lt;- workfile %&gt;% left_join(indexlegend) kable(head(workfile)) acci_id acci_time Incident_type severity acci_x acci_y severity_ID 3545008155 2019-06-10 11:59:50 حادث دهس امراة بليغ 25.28002 55.35302 3 3545009716 2019-06-10 12:04:19 حادث صدم رصيف بسيط 25.25702 55.29077 1 3545011689 2019-06-10 12:09:48 حادث صدم جدار بسيط 25.17389 55.40356 1 3545013868 2019-06-10 12:18:18 حادث ضد مجهول بسيط 25.26867 55.32277 1 3544995157 2019-06-10 11:22:00 حادث اصطدام بين سيارتين بسيط 25.26062 55.31896 1 3545003866 2019-06-10 11:47:55 حادث صدم جدار بسيط 25.08618 55.40152 1 4.2 Making the Most out of Things Sometimes, there is only so much that can be done regarding problems in datasets. In such circumstances, it is important to make the most of what is available. In this case, I would would use the packages httr and JSONlite to import data from various API’s such as OpenStreetMap and AccuWeather, and add the data to new variables such as weather_condition and road_feature. I would use the given coordinates to add road features and assign it to a new variable, or use the dates provided to get the weather on a given date and add it to the data. By doing these things, I’m able to increase coverage massively. 4.3 Maximizing the effectiveness of data By setting better standards, and trying to make the most out of what is provided, we were able to maximize the effectiveness of data. Because of that, our data has become much more useful than it has been initially. "],
["final-words.html", "Chapter 5 Final Words", " Chapter 5 Final Words Thank you for reading. All of the code used here is my own personal work. I hope you have a better understanding about how important public data is, and how better data can help improve insights significantly. "]
]
